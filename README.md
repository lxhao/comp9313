comp9313
Project1 题目很简单，了解hadoop streaming指定key和排序参数就可以完成作业。

Project2 题目不难，查阅文档，了解dataframe个rdd各种api的使用就可以完成作业。

Project3

9313最优挑战的一个作业

思路：

business的数量很多的时候，使用传统的方式根据浏览过business的共同用户，穷举business对，然后计算相似度，计算量非常大。 根据论文的方法，可以按如下方式优化：

1、举一个例子说明，如果两组单词的长度为10，相似度要大于0.3，那么把单词根据总体频率从低到高排序后，在前面7个单子中，必定有一个共同单词，根据这个思路，只需要拿出前面出现频率低的7个单词去计算候选对。 从而大量节省计算量。

2、根据上一步的计算拿到候选对后，如果两组单词的相似度要大于0.3，对两组单词的长度也有要求，这里也可以剪枝

3、计算Jaccard 相似度的时候，可以用归并排序中merge的方式节省计算量

按着3步优化后，效率可以达到top10%，满分。

支持作业、考试辅导、代写，具体可以加微信lxhao580，老师直接接单，不经过中介平台，价格优惠，服务靠谱


## **COMP9313 23T2 Project 1**** (12 marks)**

## **Problem statement:**

### Detecting popular and trending topics from news articles is important for public opinion monitoring. In this project, your task is to perform text data analysis over a dataset of Australian news from ABC (Australian Broadcasting Corporation) using **MRJob**. The problem is to compute the weights of each term regarding each year in the news articles dataset and find out the most important terms in each year whose weights are larger than a given threshold.

### **Input files:**

The dataset you are going to use contains data on news headlines published over several years. In this text file, each line is a headline of a news article, in the format of "date,term1 term2 ... ... ". The date and text are separated by a comma, and the terms are separated by a space character. A sample file is like below (note that the stop words like “to”, “the”, and “in” have already been removed from the dataset):

| 

20191124,woman stabbed adelaide shopping centre

20191204,economy continue teetering edge recession

20200401,coronanomics learnt coronavirus economy

20200401,coronavirus home test kits selling chinese community

20201015,coronavirus pacific economy foriegn aid china

20201016,china builds pig apartment blocks guard swine flu

20211216,economy starts bounce unemployment

20211224,online shopping rise due coronavirus

20211229,china close encounters elon musks

 |

This small sample file can be downloaded at:<u> https://webcms3.cse.unsw.edu.au/COMP9313/23T2/resources/88346</u>

### **Term weights computation:**

To compute the weight for a term regarding a year, please use the TF/IDF model. Specifically, the TF and IDF can be computed as:

· TF(term t, year y) = the frequency of t in y

· IDF(term t, dataset D) = log<sub>10</sub> (the number of years in D/the number of years having t)

Finally, the term weight of term t regarding the year y is computed as:

· Weight(term t, year y, dataset D) = TF(term t, year y)* IDF(term t, dataset D)

Please import math and use math.log10() to compute the term weights.

### **Code format:**

Please name your Python file “project1.py” and compress it in a package named “zID_proj1.zip” (e.g. z5123456_proj1.zip). The code template can be downloaded at: [<u>https://webcms3.cse.unsw.edu.au/COMP9313/23T2/resources/88347</u>](https://webcms3.cse.unsw.edu.au/COMP9313/23T2/resources/88347).

### **Command of running your code:**

To reduce the difficulty of the project, you are allowed to pass the total number of years to your job. We will also use more than 1 reducer to test your code. Assuming there are 20 years, β is set to 0.5, and we use 2 reducers, we will use the command like below to run your code:

**$****python3 project1.py -r hadoop hdfs_input -o hdfs_output --jobconf myjob.settings.years=20 --jobconf myjob.settings.beta=0.5 --jobconf mapreduce.job.reduces=2**

· **hdfs_input:** input file in HDFS, e.g., “hdfs://localhost:9000/user/comp9313/input”

· **hdfs_output:** output folder in HDFS, e.g., “hdfs://localhost:9000/user/comp9313/output”

· You can access the total number of years and the value of β in your program like “**N = jobconf_from_env('myjob.settings.years')**”, (use “**from mrjob.compat import jobconf_from_env**” in your code).

### **Output format:**

You need to output all terms whose term weights regarding each year are larger than the given threshold value β (note that one term could appear in different years). The format of each line is: “Term**\t**Year,Weight”. You need to sort the results first by the terms in **alphabetical**order and then by the years in **descending** order.

For example, given the above data set and β=0.4, the output can be checked at (there is no need to remove the quotation marks which are generated by MRJob): [<u>https://webcms3.cse.unsw.edu.au/COMP9313/23T2/resources/88345</u>](https://webcms3.cse.unsw.edu.au/COMP9313/23T2/resources/88345).

## **Submission:**

Deadline: Monday 26th June 11:59:59 PM

If you need an extension, please apply for a special consideration via “myUNSW” first. You need to submit through Moodle. If you submit your assignment more than once, the last submission will replace the previous one. To prove successful submission, please take a screenshot as the assignment submission instructions show and keep it by yourself. If you have any problems with submissions, please email [<u>siqing.li@unsw.edu.au</u>](mailto:siqing.li@unsw.edu.au).  

## **Late submission penalty**

5% reduction of your marks for up to 5 days, submissions delayed for over 5 days will be rejected.

## **Marking Criteria:**

Your source code will be inspected and marked based on readability and ease of understanding. Your source code's documentation (comments on the codes) is also important. Below is an indicative marking scheme:

| 

Submission can be compiled on Hadoop: 4

 |
| 

Submission can obtain correct results on a single reducer: 1

 |
| 

Submission can obtain correct results on multiple reducers: 2

 |
| 

Submission uses the combiner or in-mapper combining: 1

 |
| 

Submission uses the order inversion: 1

 |
| 

Submission uses the secondary sort: 1

 |
| 

Submission uses only a single MRStep: 1

 |
| 

Code format and structure, Readability, and Documentation: 1

 |

### **Cautions:**

· Source code that can only be compiled in the local environment is not acceptable.

· Your source code must be able to run with the provided command on Hadoop. Otherwise, it will be treated as compiling unsuccessfully.

· Please provide sufficient comments for your source code so that we can easily mark your readability and documentation. Detailed comments are not mandatory, but it is suggested that your comments could clearly describe the main logic of your source code. For example, your comments should explicitly guide the tutors to find out how you implement the combiner, sorting, etc.

· In the output file, do not include any additional “space” between the fields. In each line, only **“\t”**, **“,”**, “;” are the valid separators.

· Using multiple MRSteps is allowed but you will lose 1 mark.

## **Plagiarism****:**

The work you submit must be your own work. Submission of work partially or completely derived from any other person or jointly written with any other person is not permitted. The penalties for such an offense may include negative marks, automatic failure of the course, and possibly other academic discipline. Assignment submissions will be examined manually.     Relevant scholarship authorities will be informed if students holding scholarships are involved in an incident of plagiarism or other misconduct.     Do not provide or show your assignment work to any other person - apart from the teaching staff of this subject. If you knowingly provide or show your assignment work to another person for any reason, and work derived from it is submitted you may be penalized, even if the work was submitted without your knowledge or consent.



## **Problem statement:**

In this problem, we are still going to use the dataset of Australian news from ABC. Your task is to find out the top-*k* most frequent co-occurring term pairs in each year. The co-occurrence of (w, u) is defined as: u and w appear in the same article headline(i.e., (w, u) and (u, w) are treated equally).

### **Input files:**

The dataset you are going to use contains data of news headlines published over several years. In this text file, each line is a headline of a news article, in format of "date,term1 term2 ... ... ". The date and texts are separated by a comma, and the terms are separated by the space character. A sample file is like below:

| 

20030219,council chief executive fails to secure position

20030219,council welcomes ambulance levy decision

20030219,council welcomes insurance breakthrough

20030219,fed opp to re introduce national insurance

20040501,cowboys survive eels comeback

20040501,cowboys withstand eels fightback

20040502,castro vows cuban socialism to survive bush

20200401,coronanomics things learnt about how coronavirus economy

20200401,coronavirus at home test kits selling in the chinese community

20200401,coronavirus campbell remess streams bear making classes

20201015,coronavirus pacific economy foriegn aid china

20201016,china builds pig apartment blocks to guard against swine flu

 |

This small sample file can be downloaded at:

<u>https://webcms3.cse.unsw.edu.au/COMP9313/23T2/resources/88352</u>

### **Output format****:**

You need to ignore the stop words such as “to”, “the”, “in”, etc. (refer to the broadcast variable on how to do this efficiently). A stop word list is stored in this file:

[<u>https://webcms3.cse.unsw.edu.au/COMP9313/23T2/resources/88354</u>](https://webcms3.cse.unsw.edu.au/COMP9313/23T2/resources/88354) 

Please get the terms from the dataset as below:

· Split the headline by the space character to obtain terms.

· Ignore the stop words such as “to”, “the”, “in”, etc.

· Ignore terms starting with non-alphabetical characters, i.e., only consider terms starting with “a”to “z”.

Your Spark program should generate a list of (***k*** * *total* *years*) results, each of which is in format of “Year**\t**Term<sub>1</sub>**,**Term<sub>2</sub>**:**Count” (the two terms are sorted in alphabetical order and separated by “***,***”). The results should be first ranked by the year in ascending order, and then by the co-occurrence count of a pair in descending order, and finally by the term pair in alphabetical order.

Given *k* = 1 and the sample dataset, the output is like:

| 

2003**\t**council***,***welcomes**:**2

2004**\t**cowboys***,***eels**:**2

2020**\t**coronavirus***,***economy**:**2

 |

### **Code format:**

Please name your two python files as “project2_rdd.py” and “project2_df.py” for using RDD and DataFrame APIs, respectively. Compress it in a package named “zID_proj2.zip” (e.g. z5123456_proj2.zip).

### **Com****mand of running your code****:**

We will use the following command to run your code:

**$****spark-submit project2_rdd.py input output stopwords k**

In this command, **input** is the input file, **output** is the output folder, **stopwords** is the stop words file, and **k**is the number of pairs returned for each year.

Notes:

· You can read the files from either HDFS or the local file system. Using the local files is more convenient, but you need to use the prefix "file:///...". Spark uses HDFS by default if the path does not have a prefix.

· Please do not use numpy or pandas, since we aim to assess your understanding of the RDD/DataFrame APIs.

· You can use coalesce(1) to merge the data into a single partition and then save the data to disk.

· In the DataFrame solution, please do not use the spark.sql() function to pass the SQL statement to Spark directly.

· It does not matter if you have a new line at the end of the output file or not. It will not affect the correctness of your solution.

## **Marking Criteria:**

Your source code will be checked and marked based on readability and ease of understanding. Each solution has 8 marks. Please ensure that the code you submit can be compiled. Below is an indicative marking scheme (for each):

| 

Submission can be compiled and run on Spark: 3

 |
| 

Submission can obtain correct results: 3

· Correct term pairs

· Correct counts

· Correct order

· Correct format

· Correctly passing self-defined functions to Spark

· Correctly using Spark APIs (RDD/DataFrame solution only RDD/DataFrame APIs allowed)

 |
| 

Efficiency of top-k computation: 1

 |
| 

Efficient stop words removal: 0.5

 |
| 

Code format and structure, Readability, and Documentation: 0.5

 |

## **S****ubmission:**

Deadline: Sunday 16th Jul 11:59:59 PM

You can submit through Moodle:

If you submit your assignment more than once, the last submission will replace the previous one. To prove successful submission, please take a screenshot as assignment submission instructions show and keep it by yourself. If you have any problems in submissions, please email to [<u>siqing.li@unsw.edu.au</u>](mailto:siqing.li@unsw.edu.au).  

## **Late submission penalty**

5% reduction of your marks for up to 5 days

## **Plagiarism****:**

The work you submit must be your own work. Submission of work partially or completely derived from any other person or jointly written with any other person is not permitted. The penalties for such an offence may include negative marks, automatic failure of the course and possibly other academic discipline. Assignment submissions will be examined manually.     Relevant scholarship authorities will be informed if students holding scholarships are involved in an incident of plagiarism or other misconduct.     Do not provide or show your assignment work to any other person - apart from the teaching staff of this subject. If you knowingly provide or show your assignment work to another person for any reason, and work derived from it is submitted you may be penalized, even if the work was submitted without your knowledge or consent.


Finding Similar News Article Headlines Using Pyspark
In this problem, we are still going to use the dataset of Australian news from ABC. Similar news may appear in different years. Your task is to find all similar news article headline pairs across different years. 
Background: Set similarity self-join
Given a collection of records R, a similarity function sim(., .), and a threshold τ, the set similarity self-join on R, is to find all record pairs r and s from R, such that sim(r, s) >= τ. In this project, you are required to use the Jaccard similarity function to compute sim(r, s). Given the following example, and set τ=0.5, 
id	record
0
1
2
3
4
5	1 4 5 6
2 3 6
4 5 6
1 4 6
2 5 6
3 5
The result pairs are: 
pair	similarity
(0,2)
(0,3)
(1,4)
(2,3)
(2,4)	0.75
0.75
0.5
0.5
0.5
